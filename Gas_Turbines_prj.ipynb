{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59c9062d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       AT      AP      AH    AFDP    GTEP     TIT    TAT     TEY     CDP  \\\n",
      "0  6.8594  1007.9  96.799  3.5000  19.663  1059.2  550.0  114.70  10.605   \n",
      "1  6.7850  1008.4  97.118  3.4998  19.728  1059.3  550.0  114.72  10.598   \n",
      "\n",
      "       CO     NOX  \n",
      "0  3.1547  82.722  \n",
      "1  3.2363  82.776  \n",
      "Iteration 1, loss = 9137.93130596\n",
      "Iteration 2, loss = 9061.99992564\n",
      "Iteration 3, loss = 8919.80804246\n",
      "Iteration 4, loss = 8634.66950049\n",
      "Iteration 5, loss = 8076.20622217\n",
      "Iteration 6, loss = 7224.06202852\n",
      "Iteration 7, loss = 6163.78166689\n",
      "Iteration 8, loss = 4989.88098585\n",
      "Iteration 9, loss = 3835.35418322\n",
      "Iteration 10, loss = 2831.45005482\n",
      "Iteration 11, loss = 2059.37410236\n",
      "Iteration 12, loss = 1523.50118242\n",
      "Iteration 13, loss = 1174.04568461\n",
      "Iteration 14, loss = 941.54810041\n",
      "Iteration 15, loss = 777.23553674\n",
      "Iteration 16, loss = 653.73671062\n",
      "Iteration 17, loss = 557.09036139\n",
      "Iteration 18, loss = 480.60729765\n",
      "Iteration 19, loss = 418.80868766\n",
      "Iteration 20, loss = 368.80575804\n",
      "Iteration 21, loss = 327.84997586\n",
      "Iteration 22, loss = 294.43875350\n",
      "Iteration 23, loss = 266.93860776\n",
      "Iteration 24, loss = 244.24708842\n",
      "Iteration 25, loss = 225.23519886\n",
      "Iteration 26, loss = 209.05601209\n",
      "Iteration 27, loss = 195.27253735\n",
      "Iteration 28, loss = 183.34054713\n",
      "Iteration 29, loss = 172.81889124\n",
      "Iteration 30, loss = 163.46315852\n",
      "Iteration 31, loss = 155.10436573\n",
      "Iteration 32, loss = 147.60231392\n",
      "Iteration 33, loss = 140.67598564\n",
      "Iteration 34, loss = 134.37237899\n",
      "Iteration 35, loss = 128.57089921\n",
      "Iteration 36, loss = 123.20306078\n",
      "Iteration 37, loss = 118.24347521\n",
      "Iteration 38, loss = 113.61531592\n",
      "Iteration 39, loss = 109.21850693\n",
      "Iteration 40, loss = 105.06964941\n",
      "Iteration 41, loss = 101.16607111\n",
      "Iteration 42, loss = 97.48229630\n",
      "Iteration 43, loss = 93.92064979\n",
      "Iteration 44, loss = 90.56802670\n",
      "Iteration 45, loss = 87.43567286\n",
      "Iteration 46, loss = 84.43469029\n",
      "Iteration 47, loss = 81.56884669\n",
      "Iteration 48, loss = 78.77434666\n",
      "Iteration 49, loss = 76.10924134\n",
      "Iteration 50, loss = 73.51163295\n",
      "Iteration 51, loss = 70.99487381\n",
      "Iteration 52, loss = 68.59191714\n",
      "Iteration 53, loss = 66.23007527\n",
      "Iteration 54, loss = 63.93274915\n",
      "Iteration 55, loss = 61.75397565\n",
      "Iteration 56, loss = 59.63072040\n",
      "Iteration 57, loss = 57.55160553\n",
      "Iteration 58, loss = 55.57755617\n",
      "Iteration 59, loss = 53.64079852\n",
      "Iteration 60, loss = 51.73638487\n",
      "Iteration 61, loss = 49.85593652\n",
      "Iteration 62, loss = 48.00887674\n",
      "Iteration 63, loss = 46.21197520\n",
      "Iteration 64, loss = 44.43975789\n",
      "Iteration 65, loss = 42.62453027\n",
      "Iteration 66, loss = 40.60889391\n",
      "Iteration 67, loss = 38.89109123\n",
      "Iteration 68, loss = 37.35035591\n",
      "Iteration 69, loss = 35.93367267\n",
      "Iteration 70, loss = 34.60958903\n",
      "Iteration 71, loss = 33.41365518\n",
      "Iteration 72, loss = 32.24461450\n",
      "Iteration 73, loss = 31.15290925\n",
      "Iteration 74, loss = 30.10936838\n",
      "Iteration 75, loss = 29.08077606\n",
      "Iteration 76, loss = 28.12582578\n",
      "Iteration 77, loss = 27.22762830\n",
      "Iteration 78, loss = 26.31356521\n",
      "Iteration 79, loss = 25.44381492\n",
      "Iteration 80, loss = 24.55038470\n",
      "Iteration 81, loss = 23.71857617\n",
      "Iteration 82, loss = 22.91127753\n",
      "Iteration 83, loss = 22.11598782\n",
      "Iteration 84, loss = 21.34977018\n",
      "Iteration 85, loss = 20.61056656\n",
      "Iteration 86, loss = 19.86398316\n",
      "Iteration 87, loss = 19.12830899\n",
      "Iteration 88, loss = 18.41387542\n",
      "Iteration 89, loss = 17.71531485\n",
      "Iteration 90, loss = 17.04192327\n",
      "Iteration 91, loss = 16.40969634\n",
      "Iteration 92, loss = 15.76805283\n",
      "Iteration 93, loss = 15.15924840\n",
      "Iteration 94, loss = 14.54207800\n",
      "Iteration 95, loss = 13.95845708\n",
      "Iteration 96, loss = 13.38234043\n",
      "Iteration 97, loss = 12.82150136\n",
      "Iteration 98, loss = 12.28409058\n",
      "Iteration 99, loss = 11.76776436\n",
      "Iteration 100, loss = 11.29640355\n",
      "Iteration 101, loss = 10.81214853\n",
      "Iteration 102, loss = 10.38901029\n",
      "Iteration 103, loss = 9.93514738\n",
      "Iteration 104, loss = 9.52706061\n",
      "Iteration 105, loss = 9.12279185\n",
      "Iteration 106, loss = 8.74168840\n",
      "Iteration 107, loss = 8.38051741\n",
      "Iteration 108, loss = 8.02487095\n",
      "Iteration 109, loss = 7.68650917\n",
      "Iteration 110, loss = 7.35275271\n",
      "Iteration 111, loss = 7.05429632\n",
      "Iteration 112, loss = 6.77267462\n",
      "Iteration 113, loss = 6.48367852\n",
      "Iteration 114, loss = 6.21575995\n",
      "Iteration 115, loss = 5.95279327\n",
      "Iteration 116, loss = 5.68650448\n",
      "Iteration 117, loss = 5.45443473\n",
      "Iteration 118, loss = 5.22640374\n",
      "Iteration 119, loss = 5.02449121\n",
      "Iteration 120, loss = 4.81901703\n",
      "Iteration 121, loss = 4.61235776\n",
      "Iteration 122, loss = 4.41965557\n",
      "Iteration 123, loss = 4.24276511\n",
      "Iteration 124, loss = 4.05600956\n",
      "Iteration 125, loss = 3.87483936\n",
      "Iteration 126, loss = 3.70432987\n",
      "Iteration 127, loss = 3.54253027\n",
      "Iteration 128, loss = 3.39631558\n",
      "Iteration 129, loss = 3.25176384\n",
      "Iteration 130, loss = 3.11688243\n",
      "Iteration 131, loss = 2.97339992\n",
      "Iteration 132, loss = 2.84295095\n",
      "Iteration 133, loss = 2.70652837\n",
      "Iteration 134, loss = 2.57571494\n",
      "Iteration 135, loss = 2.44816082\n",
      "Iteration 136, loss = 2.33372876\n",
      "Iteration 137, loss = 2.21774109\n",
      "Iteration 138, loss = 2.10746183\n",
      "Iteration 139, loss = 2.01880559\n",
      "Iteration 140, loss = 1.90991554\n",
      "Iteration 141, loss = 1.82091886\n",
      "Iteration 142, loss = 1.73350736\n",
      "Iteration 143, loss = 1.65103348\n",
      "Iteration 144, loss = 1.57469091\n",
      "Iteration 145, loss = 1.50441308\n",
      "Iteration 146, loss = 1.42709983\n",
      "Iteration 147, loss = 1.35691336\n",
      "Iteration 148, loss = 1.29553490\n",
      "Iteration 149, loss = 1.23403776\n",
      "Iteration 150, loss = 1.17183676\n",
      "Iteration 151, loss = 1.11824083\n",
      "Iteration 152, loss = 1.05946507\n",
      "Iteration 153, loss = 1.01412853\n",
      "Iteration 154, loss = 0.95641680\n",
      "Iteration 155, loss = 0.90841812\n",
      "Iteration 156, loss = 0.86065738\n",
      "Iteration 157, loss = 0.80750041\n",
      "Iteration 158, loss = 0.75773339\n",
      "Iteration 159, loss = 0.71186249\n",
      "Iteration 160, loss = 0.66880086\n",
      "Iteration 161, loss = 0.63129167\n",
      "Iteration 162, loss = 0.58726562\n",
      "Iteration 163, loss = 0.55263391\n",
      "Iteration 164, loss = 0.52067533\n",
      "Iteration 165, loss = 0.49008329\n",
      "Iteration 166, loss = 0.45685302\n",
      "Iteration 167, loss = 0.43381383\n",
      "Iteration 168, loss = 0.41486703\n",
      "Iteration 169, loss = 0.39436193\n",
      "Iteration 170, loss = 0.38066688\n",
      "Iteration 171, loss = 0.36295852\n",
      "Iteration 172, loss = 0.35396227\n",
      "Iteration 173, loss = 0.34721279\n",
      "Iteration 174, loss = 0.33784944\n",
      "Iteration 175, loss = 0.33192618\n",
      "Iteration 176, loss = 0.32857078\n",
      "Iteration 177, loss = 0.32048691\n",
      "Iteration 178, loss = 0.31557886\n",
      "Iteration 179, loss = 0.31187166\n",
      "Iteration 180, loss = 0.30922323\n",
      "Iteration 181, loss = 0.30527377\n",
      "Iteration 182, loss = 0.30051553\n",
      "Iteration 183, loss = 0.29902708\n",
      "Iteration 184, loss = 0.29493831\n",
      "Iteration 185, loss = 0.29294781\n",
      "Iteration 186, loss = 0.29472212\n",
      "Iteration 187, loss = 0.29033479\n",
      "Iteration 188, loss = 0.28240374\n",
      "Iteration 189, loss = 0.27844171\n",
      "Iteration 190, loss = 0.27515945\n",
      "Iteration 191, loss = 0.27399688\n",
      "Iteration 192, loss = 0.27239552\n",
      "Iteration 193, loss = 0.27284508\n",
      "Iteration 194, loss = 0.27009001\n",
      "Iteration 195, loss = 0.26603717\n",
      "Iteration 196, loss = 0.26827853\n",
      "Iteration 197, loss = 0.26756197\n",
      "Iteration 198, loss = 0.27267665\n",
      "Iteration 199, loss = 0.26469259\n",
      "Iteration 200, loss = 0.26390692\n",
      "Mean Squared Error: 0.4962918428764163\n",
      "R-squared: 0.9980179216399153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grish\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"C:/Users/grish/Downloads/Assignment/Assignment/Project  - 12 Neural Network/gas_turbines.csv\")\n",
    "print(data.head(2))\n",
    "\n",
    "# Define features (ambient variables) and target variable (TEY)\n",
    "X = data[['AT','AP','AH','AFDP','GTEP','TIT','TAT','CDP','CO','NOX']]\n",
    "y = data['TEY']\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train the neural network regressor\n",
    "model = MLPRegressor(hidden_layer_sizes=(6,5),random_state=42,verbose=True)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict turbine energy yield (TEY) using the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963e6e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
